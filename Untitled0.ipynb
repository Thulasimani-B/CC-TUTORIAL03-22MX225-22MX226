{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dne63b8WgfX0JX_0KdeDy0ddmB72BcRd",
      "authorship_tag": "ABX9TyP92Apj7dX3jRsf0Vg+MBdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thulasimani-B/CC-TUTORIAL03-22MX225-22MX226/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pdf2image\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install python-poppler\n",
        "!pip install PyMuPDF\n",
        "!pip install Aspose.Email-for-Python-via-NET\n",
        "!pip install aspose-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEwZ09tIDwBx",
        "outputId": "74c4f21b-a8ef-47ba-c0f6-d9fbe6a23bfe"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.18.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 109 kB in 4s (29.6 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-poppler\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.0 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.0)\n",
            "Requirement already satisfied: Aspose.Email-for-Python-via-NET in /usr/local/lib/python3.10/dist-packages (23.7)\n",
            "Requirement already satisfied: aspose-words in /usr/local/lib/python3.10/dist-packages (23.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmmRetN4EbDp",
        "outputId": "813e7548-2233-4f37-f3e1-0968aca17e83"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDhzhHQTUj0H",
        "outputId": "7946e5a2-36d1-4f59-968a-fdc4161b1445"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "UT2yd6zR0zvA"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "import numpy as np\n",
        "from itertools import groupby, count\n",
        "import re\n",
        "import subprocess\n",
        "import os.path\n",
        "import sys\n",
        "import logging\n",
        "import joblib\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import pytesseract\n",
        "import cv2\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "import aspose.words as aw\n",
        "import fitz\n",
        "logger_watchtower = logging.getLogger(__name__)\n",
        "from pandas.errors import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
      ],
      "metadata": {
        "id": "tMOW03L18Bs2"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_100.csv file contains 100 tech blogs links so we can use it for web scraping of technical words\n",
        "df=pd.read_csv('/content/data_100.csv')\n",
        "\n",
        "#splits the words in the sentences\n",
        "sent = [row.split() for row in df['data']]\n",
        "\n",
        "#extracts the words that are repeated atleast of 30 times\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "\n",
        "#The phrases[sent] syntax uses the trained phrases object to transform the individual word lists into sentences with detected phrases.\n",
        "sentences=phrases[sent]\n",
        "\n",
        "# for sentence in sentences:\n",
        "#   print(sentence)"
      ],
      "metadata": {
        "id": "rxuTROTt06y6"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "window=3,\n",
        "vector_size=300,\n",
        "sample=6e-5,\n",
        "alpha=0.03,\n",
        "min_alpha=0.0007,\n",
        "negative=20\n",
        ")\n",
        "\n",
        "#Building Vocabulary\n",
        "w2v_model.build_vocab(sentences)\n",
        "\n",
        "#Storing the built vocabulary\n",
        "vocabulary = w2v_model.wv.index_to_key\n",
        "\n",
        "#Training the model\n",
        "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
        "\n",
        "#saving the model\n",
        "path = \"/content/drive/MyDrive/w2v_model.joblib\"\n",
        "joblib.dump(w2v_model, path)\n",
        "\n",
        "#verifying the model with similarity check\n",
        "print(w2v_model.wv.similarity('artificial_intelligence', 'machine_learning'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU41CC5Q5GB6",
        "outputId": "ab9f794c-7b02-4569-9a14-b5a025ebc817"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7528914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _skills_in_box(image_gray,threshold=60):\n",
        "\n",
        "  #Function for identifying boxes and identifying skills in it: it returns the string in the box\n",
        "  img = image_gray.copy()\n",
        "  thresh_inv = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  # Blurring the image for noise reduction\n",
        "  blur = cv2.GaussianBlur(thresh_inv,(1,1),0)\n",
        "  thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  # find contours using OpenCV\n",
        "  contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "  mask = np.ones(img.shape[:2], dtype=\"uint8\") * 255\n",
        "  available = 0\n",
        "  for c in contours:\n",
        "    # get the bounding rect\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    if w*h>1000:\n",
        "        cv2.rectangle(mask, (x+5, y+5), (x+w-5, y+h-5), (0, 0, 255), -1)\n",
        "        available = 1\n",
        "\n",
        "  res = ''\n",
        "  if available == 1:\n",
        "    res_final = cv2.bitwise_and(img, img, mask=cv2.bitwise_not(mask))\n",
        "    res_final[res_final<=threshold]=0\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
        "    res_fin = cv2.filter2D(src=res_final, ddepth=-1, kernel=kernel)\n",
        "    vt = pytesseract.image_to_data(255-res_final,output_type='data.frame')\n",
        "    vt = vt[vt.conf != -1]\n",
        "    res = ''\n",
        "    for i in vt[vt['conf']>=43]['text']:\n",
        "      res = res + str(i) + ' '\n",
        "  print(res)\n",
        "  return res\n",
        "\n",
        "def _image_to_string(img):\n",
        "\n",
        "  #Function for converting images to grayscale and converting to text returns text in it.\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  res = ''\n",
        "  string1 = pytesseract.image_to_data(img,output_type='data.frame')\n",
        "  string1 = string1[string1['conf'] != -1]\n",
        "\n",
        "  #selecting the text with confidence above 43%\n",
        "  for i in string1[string1['conf']>=43]['text']:\n",
        "    res = res + str(i) + ' '\n",
        "  string3 = _skills_in_box(img)\n",
        "  return res+string3\n",
        "\n",
        "def _pdf_to_png(pdf_path):\n",
        "\n",
        "    #Function for converting pdf to image and saves it in a folder and convert the image into string\n",
        "\n",
        "    string = ''\n",
        "    images = convert_from_path(pdf_path)\n",
        "    for j in tqdm(range(len(images))):\n",
        "        # Save pages as images in the pdf\n",
        "        image = np.array(images[j])\n",
        "        string += _image_to_string(image)\n",
        "        string += '\\n'\n",
        "    return string\n",
        "\n",
        "\n",
        "def ocr(paths):\n",
        "\n",
        "    #Function for checking the pdf is image or not. if pdf is given it will be converted to image\n",
        "\n",
        "    text = \"\"\n",
        "    res = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(paths)\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        if len(text) <=10 :\n",
        "            res = _pdf_to_png(paths)\n",
        "        else:\n",
        "            res = text\n",
        "    except:\n",
        "        doc = aw.Document(paths)\n",
        "        doc.save(\"Document.pdf\")\n",
        "        doc = fitz.open(\"Document.pdf\")\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        if len(text) <=10 :\n",
        "            res = _pdf_to_png(\"Document.pdf\")\n",
        "        else:\n",
        "            res = text\n",
        "        os.remove(\"Document.pdf\")\n",
        "    return res"
      ],
      "metadata": {
        "id": "fyoWppfhGj3i"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding cosine similarities\n",
        "\n",
        "def to_la(L):\n",
        "  k=list(L)\n",
        "  l=np.array(k)\n",
        "  return l.reshape(-1, 1)\n",
        "\n",
        "def cos(A, B):\n",
        "  dot_prod=np.matmul(A,B.T)\n",
        "  norm_a=np.reciprocal(np.sum(np.abs(A)**2,axis=-1)**(1./2))\n",
        "  norm_b=np.reciprocal(np.sum(np.abs(B)**2,axis=-1)**(1./2))\n",
        "  norm_a=to_la(norm_a)\n",
        "  norm_b=to_la(norm_b)\n",
        "  k=np.matmul(norm_a,norm_b.T)\n",
        "  return list(np.multiply(dot_prod,k))"
      ],
      "metadata": {
        "id": "WQdxjDGyHt86"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to process resume's text and compares with job description: it returns dict of skills and binary representation of presence\n",
        "\n",
        "def check(path,skills,l2,w2v_model1,phrases,pattern):\n",
        "  text = ocr(path)\n",
        "  text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"\\\\\\|,|/|:|\\)|\\(\",\" \",text)\n",
        "  t2 = text.split()\n",
        "  l_2=l2.copy()\n",
        "  match=list(set(re.findall(pattern,text)))\n",
        "  sentences=phrases[t2]\n",
        "  resume_skills_dict={}\n",
        "  res_jdskill_intersect=list(set(sentences).intersection(set(l_2)))\n",
        "  if(len(match)!=0):\n",
        "    for k in match:\n",
        "      k=k.replace(' ','_')\n",
        "      resume_skills_dict[k]=1\n",
        "      try:\n",
        "        l_2.remove(k)\n",
        "      except:\n",
        "        continue\n",
        "  l6=list(set(l_2).intersection(skills[0]))\n",
        "  l6_minus_skills=list(set(l_2).difference(skills[0]))\n",
        "  for i in l6_minus_skills:\n",
        "    resume_skills_dict[i]=0\n",
        "  if(len(l6)==0):\n",
        "    return resume_skills_dict\n",
        "  l4=list(set(sentences).intersection(skills[0]))\n",
        "  arr1=np.array([w2v_model1[i] for i in l6])\n",
        "  arr2=np.array([w2v_model1[i] for i in l4])\n",
        "  similarity_values=cos(arr1,arr2)\n",
        "  count=0\n",
        "  for i in similarity_values:\n",
        "    k=list(filter(lambda x: x<0.38, list(i)))\n",
        "    if(len(k)==len(i)):\n",
        "      resume_skills_dict[l6[count]]=0\n",
        "    else:\n",
        "      resume_skills=[s for s in range(len(i)) if(i[s])>0.38]\n",
        "      resume_skills_dict[l6[count]]=1\n",
        "    count+=1\n",
        "  return resume_skills_dict"
      ],
      "metadata": {
        "id": "5ZgYNSlEHyeA"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Convert(string):\n",
        "    li = list(string.split())\n",
        "    return list(set(li))\n",
        "\n",
        "def preprocess(string):\n",
        "  string = string.replace(\",\",' ')\n",
        "  string= string.replace(\"'\",' ')\n",
        "  string = Convert(string)\n",
        "  return string"
      ],
      "metadata": {
        "id": "0gIuo_fMH-YQ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   #Arg 1 = vocabulary, Arg 2 = model, Arg 3 = JD's Mandatory Skills, Arg 4 = Resume Path\n",
        "   argv = [vocabulary,\n",
        "           '/content/drive/MyDrive/w2v_model.joblib',\n",
        "            'ml, mysql, oracle, python, pytorch, r, scikit learn, snowflake, sql, tensorflow',\n",
        "           '/content/RESUME_THULASIMANI_B.pdf']\n",
        "   w2v_model1 = joblib.load(argv[1])\n",
        "   encoding='latin-1'\n",
        "   skills=vocabulary\n",
        "\n",
        "   mapper = {}\n",
        "   underscore=[]\n",
        "   jd_skills=argv[2]\n",
        "   jd_skills=\" \".join(jd_skills.strip().split())\n",
        "   jd_skills=jd_skills.replace(', ',',')\n",
        "   pattern=jd_skills.replace(',','|').lower()\n",
        "   for i in jd_skills.split(','):\n",
        "    if '_' in i:\n",
        "      underscore.append(i)\n",
        "      mapper[i.lower().replace('_',' ')] = i\n",
        "   jd_skills=jd_skills.replace(' ','_')\n",
        "   jd_skills=jd_skills.replace(',',', ')\n",
        "   for i in jd_skills.split(', '):\n",
        "    if i not in underscore:\n",
        "      if '_' in i:\n",
        "        mapper[i.lower().replace('_',' ')] = i.replace('_',' ')\n",
        "      elif '-' in i:\n",
        "        mapper[i.lower().replace('-',' ')] = i\n",
        "      else:\n",
        "        mapper[i.lower()] = i\n",
        "   jd_skills=jd_skills.replace('-','_')\n",
        "\n",
        "   lines = [preprocess(jd_skills.lower().rstrip())]\n",
        "\n",
        "   final_jd_skills=list(set(lines[0]).intersection(skills[0]))\n",
        "   path = argv[3]\n",
        "   res=check(path,skills,lines[0],w2v_model1,phrases,pattern)\n",
        "   print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5FI6lCYICHQ",
        "outputId": "a2aeb760-8a1f-4e9f-906f-906668df5c2a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sql': 1, 'python': 1, 'oracle': 1, 'r': 1, 'scikit_learn': 0, 'pytorch': 0, 'ml': 0, 'tensorflow': 0, 'snowflake': 0, 'mysql': 0}\n"
          ]
        }
      ]
    }
  ]
}